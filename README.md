# Image-Captioning-on-Android-app
Visually impaired humans are usually unrealizable of the danger that they face in their daily life. They may face several obstacles once performing their daily activities. This project introduces an aid impaired people system which helps visually impaired people to recognize the environment surrounding them. It helps them to detect and recognize the tools around them, which they see through a mobile camera. It is connected to a system that recognizes the environment and provides the output as a voice message. The system imitates the human eye. A mobile camera captures an image as an input for the model that employs a neural network for recognition of pre-trained objects with a considerable accuracy on the Microsoft Common Objects (MS-COCO) dataset. The system will return the name of the object as an audio. The process of analysis using long complex algorithms known as the convolutional neural network algorithms. The algorithm analyzes the images into parts to compare them with the most important features of the objects in the pictures related to the dataset. The description of the captured image generated by LSTM and result will be converted to a voice message to tell the visually impaired people about the environment in front of them. Also, we developed a hardware device which calculates the distance between visually impaired person and the objects to alert him.
